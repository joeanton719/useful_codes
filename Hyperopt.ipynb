{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HYPEROPT \n",
    "\n",
    "Hyperopt codes for majot machine learning models (Classifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# hp: define the hyperparameter space\n",
    "# fmin: optimization function\n",
    "# Trials: to evaluate the different searched hyperparameters\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from hyperopt import hp, fmin, Trials\n",
    "\n",
    "# the search algorithms\n",
    "from hyperopt import rand, anneal, tpe\n",
    "\n",
    "# for the search\n",
    "from hyperopt import STATUS_OK, STATUS_FAIL\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': hp.quniform('n_estimators', 10, 2500, 25),\n",
    "    'max_depth': hp.quniform('max_depth', 1, 7, 1),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.001), np.log(1)),\n",
    "    'booster': hp.choice('booster', ['gbtree', 'dart', 'gblinear']),\n",
    "    'gamma': hp.loguniform('gamma', np.log(0.01), np.log(10)),\n",
    "    'subsample': hp.uniform('subsample', 0.50, 0.90),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.50, 0.99),\n",
    "    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.50, 0.99),\n",
    "    'colsample_bynode': hp.uniform('colsample_bynode', 0.50, 0.99),\n",
    "    'reg_lambda': hp.loguniform('reg_lambda', np.log(1), np.log(20)), \n",
    "    'min_child_weight' : hp.quniform('min_child_weight', 1, 20, 1), \n",
    "    'base_score': hp.uniform('base_score', 0.3, 0.65), \n",
    "    'max_delta_step':  hp.loguniform('max_delta_step', np.log(0.01), np.log(10)), \n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0.0001, 2.0)\n",
    "}\n",
    "\n",
    "#Defining Objective Function\n",
    "\n",
    "def objective(params):\n",
    "\n",
    "    # we need a dictionary to indicate which value from the space\n",
    "    # to attribute to each value of the hyperparameter in the xgb\n",
    "    params_dict = {\n",
    "        # important int, as it takes integers only\n",
    "        'n_estimators': int(params['n_estimators']),\n",
    "        # important int, as it takes integers only\n",
    "        'max_depth': int(params['max_depth']),\n",
    "        'learning_rate': params['learning_rate'],\n",
    "        'booster': params['booster'],\n",
    "        'gamma': params['gamma'],\n",
    "        'subsample': params['subsample'],\n",
    "        'colsample_bytree': params['colsample_bytree'],\n",
    "        'colsample_bylevel': params['colsample_bylevel'],\n",
    "        'colsample_bynode': params['colsample_bynode'],\n",
    "        'random_state': seed,\n",
    "        'reg_lambda': int(params['reg_lambda']), \n",
    "        'min_child_weight': int(params['min_child_weight']),\n",
    "        'base_score': params['base_score'], \n",
    "        'max_delta_step': int(params['max_delta_step']), \n",
    "        'reg_lambda': int(params['reg_lambda'])\n",
    "    }\n",
    "\n",
    "    # with ** we pass the items in the dictionary as parameters\n",
    "    # to the xgb\n",
    "    model = XGBClassifier(**params_dict)\n",
    "\n",
    "    # train with cv\n",
    "    cross_val_data = cross_val_score(\n",
    "        model, \n",
    "        X_train_pars, \n",
    "        y_train,\n",
    "        scoring='roc_auc', \n",
    "        cv=StratifiedKFold(n_splits=10, random_state=seed, shuffle=True), \n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # === IMPORTANT ===\n",
    "    # data to be returned by the search, we can add as much as we want\n",
    "    \n",
    "    loss = -cross_val_data.mean()\n",
    "    loss_variance = cross_val_data.std()\n",
    "    \n",
    "    try:\n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'loss_variance':loss_variance,\n",
    "            'status': STATUS_OK,\n",
    "            }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'exception': str(e),\n",
    "            'status': STATUS_FAIL,\n",
    "            }\n",
    "\n",
    "\n",
    "trials_tpe = Trials()\n",
    "\n",
    "tpe_search = fmin(\n",
    "    fn=objective,\n",
    "    space=param_grid,\n",
    "    max_evals=120,\n",
    "    rstate=np.random.RandomState(seed),\n",
    "    algo=tpe.suggest,  # tpe\n",
    "    trials=trials_tpe\n",
    ")\n",
    "\n",
    "print()\n",
    "print(f'Best Params: {tpe_search}')\n",
    "print()\n",
    "\n",
    "#Plots\n",
    "results = pd.concat([\n",
    "    pd.DataFrame(trials_tpe.vals),\n",
    "    pd.DataFrame(trials_tpe.results)],\n",
    "    axis=1,\n",
    ").sort_values(by='loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "results['index'] = results.index\n",
    "\n",
    "ax = sns.lineplot(x='index', y='loss', data=results)\n",
    "ax.fill_between(\n",
    "    results[\"index\"],\n",
    "    y1=results[\"loss\"] - results[\"loss_variance\"],\n",
    "    y2=results[\"loss\"] + results[\"loss_variance\"],\n",
    "    alpha=.5,\n",
    ")\n",
    "plt.xlabel('interation')\n",
    "plt.title('Random Search')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To pass as dictionary\n",
    "\n",
    "def create_param_grid(search, booster): #Here, booster represents the cateogircal parameter.\n",
    "    best_hp_dict = {\n",
    "        'n_estimators': int(search['n_estimators']),\n",
    "        # important int, as it takes integers only\n",
    "        'max_depth': int(search['max_depth']),\n",
    "        'learning_rate': search['learning_rate'],\n",
    "        'booster': booster,\n",
    "        'gamma': search['gamma'],\n",
    "        'subsample': search['subsample'],\n",
    "        'colsample_bytree': search['colsample_bytree'],\n",
    "        'colsample_bylevel': search['colsample_bylevel'],\n",
    "        'colsample_bynode': search['colsample_bynode'],\n",
    "        'random_state': seed,\n",
    "        'reg_lambda': int(search['reg_lambda']), \n",
    "        'min_child_weight': int(search['min_child_weight']),\n",
    "        'base_score': search['base_score'], \n",
    "        'max_delta_step': int(search['max_delta_step']), \n",
    "        'reg_lambda': int(search['reg_lambda'])\n",
    "    }\n",
    "    return best_hp_dict\n",
    "\n",
    "chosen_param = create_param_grid(tpe_search, #Categorical columns 'gbtree')\n",
    "print(chosen_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# hp: define the hyperparameter space\n",
    "# fmin: optimization function\n",
    "# Trials: to evaluate the different searched hyperparameters\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from hyperopt import hp, fmin, Trials\n",
    "\n",
    "# the search algorithms\n",
    "from hyperopt import rand, anneal, tpe\n",
    "\n",
    "# for the search\n",
    "from hyperopt import STATUS_OK, STATUS_FAIL\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "param_grid = {\n",
    "    'iterations': hp.quniform('iterations', 200, 1000, 25),\n",
    "    'depth': hp.quniform('depth', 1, 7, 1),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.001), np.log(1)),\n",
    "    'boosting_type': hp.choice('boosting_type', ['Ordered', 'Plain']),\n",
    "    'l2_leaf_reg': hp.uniform('l2_leaf_reg', 1.0, 100.0),\n",
    "    'border_count': hp.loguniform('border_count', np.log(5), np.log(200))\n",
    "}\n",
    "\n",
    "#Defining Objective Function\n",
    "\n",
    "def objective(params):\n",
    "\n",
    "    # we need a dictionary to indicate which value from the space\n",
    "    # to attribute to each value of the hyperparameter in the xgb\n",
    "    params_dict = {\n",
    "        'iterations': int(params['iterations']),\n",
    "        'depth': int(params['depth']),\n",
    "        'learning_rate': params['learning_rate'],\n",
    "        'boosting_type': params['boosting_type'],\n",
    "        'l2_leaf_reg': params['l2_leaf_reg'],\n",
    "        'border_count': int(params['border_count']) \n",
    "    }\n",
    "\n",
    "    # with ** we pass the items in the dictionary as parameters\n",
    "    # to the xgb\n",
    "    \n",
    "    cat = CatBoostClassifier(verbose = False, loss_function='CrossEntropy', eval_metric='TotalF1')\n",
    "    \n",
    "    model = cat.set_params(**params_dict)\n",
    "\n",
    "    # train with cv\n",
    "    cross_val_data = cross_val_score(\n",
    "        model, \n",
    "        X_train_pars, \n",
    "        y_train,\n",
    "        scoring='roc_auc', \n",
    "        cv=StratifiedKFold(n_splits=10, random_state=seed, shuffle=True), \n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    \n",
    "    # === IMPORTANT ===\n",
    "    # data to be returned by the search, we can add as much as we want\n",
    "    \n",
    "    loss = -cross_val_data.mean()\n",
    "    loss_variance = cross_val_data.std()\n",
    "    \n",
    "    try:\n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'loss_variance':loss_variance,\n",
    "            'status': STATUS_OK,\n",
    "            }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'exception': str(e),\n",
    "            'status': STATUS_FAIL,\n",
    "            }\n",
    "\n",
    "\n",
    "trials_tpe = Trials()\n",
    "\n",
    "tpe_search = fmin(\n",
    "    fn=objective,\n",
    "    space=param_grid,\n",
    "    max_evals=120,\n",
    "    rstate=np.random.RandomState(seed),\n",
    "    algo=tpe.suggest,  # tpe\n",
    "    trials=trials_tpe\n",
    ")\n",
    "\n",
    "print()\n",
    "print(f'Best Params: {tpe_search}')\n",
    "print()\n",
    "\n",
    "#Plots\n",
    "results = pd.concat([\n",
    "    pd.DataFrame(trials_tpe.vals),\n",
    "    pd.DataFrame(trials_tpe.results)],\n",
    "    axis=1,\n",
    ").sort_values(by='loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "results['index'] = results.index\n",
    "\n",
    "ax = sns.lineplot(x='index', y='loss', data=results)\n",
    "ax.fill_between(\n",
    "    results[\"index\"],\n",
    "    y1=results[\"loss\"] - results[\"loss_variance\"],\n",
    "    y2=results[\"loss\"] + results[\"loss_variance\"],\n",
    "    alpha=.5,\n",
    ")\n",
    "plt.xlabel('interation')\n",
    "plt.title('Random Search')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_param_grid(search, boosting_type):\n",
    "    best_hp_dict = {\n",
    "        'iterations': int(search['iterations']),\n",
    "        'depth': int(search['depth']),\n",
    "        'learning_rate': search['learning_rate'],\n",
    "        'boosting_type': boosting_type,\n",
    "        'l2_leaf_reg': search['l2_leaf_reg'],\n",
    "        'border_count': int(search['border_count'])\n",
    "    }\n",
    "    return best_hp_dict\n",
    "\n",
    "chosen_param = create_param_grid(tpe_search, #boosting_type in string format)\n",
    "print(chosen_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# hp: define the hyperparameter space\n",
    "# fmin: optimization function\n",
    "# Trials: to evaluate the different searched hyperparameters\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from hyperopt import hp, fmin, Trials\n",
    "\n",
    "# the search algorithms\n",
    "from hyperopt import rand, anneal, tpe\n",
    "\n",
    "# for the search\n",
    "from hyperopt import STATUS_OK, STATUS_FAIL\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': hp.quniform('n_estimators', 10, 2500, 25),\n",
    "    'max_samples': hp.uniform('max_samples', 0.50, 0.999),\n",
    "    'max_features': hp.uniform('max_features', 0.30, 0.999),\n",
    "    'bootstrap': hp.choice('bootstrap', [True, False]),\n",
    "    'bootstrap_features': hp.choice('bootstrap_features', [True, False]),\n",
    "    'warm_start': hp.choice('warm_start', [True, False])\n",
    "}\n",
    "\n",
    "#Defining Objective Function\n",
    "\n",
    "def objective(params):\n",
    "\n",
    "    # we need a dictionary to indicate which value from the space\n",
    "    # to attribute to each value of the hyperparameter in the xgb\n",
    "    params_dict = {\n",
    "        # important int, as it takes integers only\n",
    "        'n_estimators': int(params['n_estimators']),\n",
    "        'max_samples': params['max_samples'],\n",
    "        'max_features': params['max_features'],\n",
    "        'bootstrap': params['bootstrap'],\n",
    "        'bootstrap_features': params['bootstrap_features'],\n",
    "        'warm_start': params['warm_start']\n",
    "    }\n",
    "\n",
    "    # with ** we pass the items in the dictionary as parameters\n",
    "    # to the xgb\n",
    "    model = BaggingClassifier(n_jobs = -1, **params_dict)\n",
    "\n",
    "    # train with cv\n",
    "    cross_val_data = cross_val_score(\n",
    "        model, \n",
    "        X_train_pars, \n",
    "        y_train,\n",
    "        scoring='roc_auc', \n",
    "        cv=StratifiedKFold(n_splits=10, random_state=seed, shuffle=True), \n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    \n",
    "    # === IMPORTANT ===\n",
    "    # data to be returned by the search, we can add as much as we want\n",
    "    \n",
    "    loss = -cross_val_data.mean()\n",
    "    loss_variance = cross_val_data.std()\n",
    "    \n",
    "    try:\n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'loss_variance':loss_variance,\n",
    "            'status': STATUS_OK,\n",
    "            }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'exception': str(e),\n",
    "            'status': STATUS_FAIL,\n",
    "            }\n",
    "\n",
    "\n",
    "trials_tpe = Trials()\n",
    "\n",
    "tpe_search = fmin(\n",
    "    fn=objective,\n",
    "    space=param_grid,\n",
    "    max_evals=120,\n",
    "    rstate=np.random.RandomState(seed),\n",
    "    algo=tpe.suggest,  # tpe\n",
    "    trials=trials_tpe\n",
    ")\n",
    "\n",
    "print()\n",
    "print(f'Best Params: {tpe_search}')\n",
    "print()\n",
    "\n",
    "#Plots\n",
    "results = pd.concat([\n",
    "    pd.DataFrame(trials_tpe.vals),\n",
    "    pd.DataFrame(trials_tpe.results)],\n",
    "    axis=1,\n",
    ").sort_values(by='loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "results['index'] = results.index\n",
    "\n",
    "ax = sns.lineplot(x='index', y='loss', data=results)\n",
    "ax.fill_between(\n",
    "    results[\"index\"],\n",
    "    y1=results[\"loss\"] - results[\"loss_variance\"],\n",
    "    y2=results[\"loss\"] + results[\"loss_variance\"],\n",
    "    alpha=.5,\n",
    ")\n",
    "plt.xlabel('interation')\n",
    "plt.title('Random Search')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_param_grid(search, bootstrap, bootstrap_features, warm_start):\n",
    "    best_hp_dict = {\n",
    "        'n_estimators': int(search['n_estimators']),\n",
    "        'max_samples': search['max_samples'],\n",
    "        'max_features': search['max_features'],\n",
    "        'bootstrap': bootstrap,\n",
    "        'bootstrap_features': bootstrap_features,\n",
    "        'warm_start': warm_start\n",
    "    }\n",
    "    return best_hp_dict\n",
    "\n",
    "chosen_param = create_param_grid(tpe_search, #bootstrap, bootstrap_features, warm_start in string format)\n",
    "print(chosen_param)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
