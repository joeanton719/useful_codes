{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HYPEROPT \n",
    "\n",
    "Hyperopt codes for majot machine learning models (Classifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# hp: define the hyperparameter space\n",
    "# fmin: optimization function\n",
    "# Trials: to evaluate the different searched hyperparameters\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from hyperopt import hp, fmin, Trials\n",
    "\n",
    "# the search algorithms\n",
    "from hyperopt import rand, anneal, tpe\n",
    "\n",
    "# for the search\n",
    "from hyperopt import STATUS_OK, STATUS_FAIL\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': hp.quniform('n_estimators', 10, 2500, 25),\n",
    "    'max_depth': hp.quniform('max_depth', 1, 7, 1),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.001), np.log(1)),\n",
    "    'booster': hp.choice('booster', ['gbtree', 'dart', 'gblinear']),\n",
    "    'gamma': hp.loguniform('gamma', np.log(0.01), np.log(10)),\n",
    "    'subsample': hp.uniform('subsample', 0.50, 0.90),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.50, 0.99),\n",
    "    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.50, 0.99),\n",
    "    'colsample_bynode': hp.uniform('colsample_bynode', 0.50, 0.99),\n",
    "    'reg_lambda': hp.loguniform('reg_lambda', np.log(1), np.log(20)), \n",
    "    'min_child_weight' : hp.quniform('min_child_weight', 1, 20, 1), \n",
    "    'base_score': hp.uniform('base_score', 0.3, 0.65), \n",
    "    'max_delta_step':  hp.loguniform('max_delta_step', np.log(0.01), np.log(10)), \n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0.0001, 2.0)\n",
    "}\n",
    "\n",
    "#Defining Objective Function\n",
    "\n",
    "def objective(params):\n",
    "\n",
    "    # we need a dictionary to indicate which value from the space\n",
    "    # to attribute to each value of the hyperparameter in the xgb\n",
    "    params_dict = {\n",
    "        # important int, as it takes integers only\n",
    "        'n_estimators': int(params['n_estimators']),\n",
    "        # important int, as it takes integers only\n",
    "        'max_depth': int(params['max_depth']),\n",
    "        'learning_rate': params['learning_rate'],\n",
    "        'booster': params['booster'],\n",
    "        'gamma': params['gamma'],\n",
    "        'subsample': params['subsample'],\n",
    "        'colsample_bytree': params['colsample_bytree'],\n",
    "        'colsample_bylevel': params['colsample_bylevel'],\n",
    "        'colsample_bynode': params['colsample_bynode'],\n",
    "        'random_state': seed,\n",
    "        'reg_lambda': int(params['reg_lambda']), \n",
    "        'min_child_weight': int(params['min_child_weight']),\n",
    "        'base_score': params['base_score'], \n",
    "        'max_delta_step': int(params['max_delta_step']), \n",
    "        'reg_lambda': int(params['reg_lambda'])\n",
    "    }\n",
    "\n",
    "    # with ** we pass the items in the dictionary as parameters\n",
    "    # to the xgb\n",
    "    model = XGBClassifier(**params_dict)\n",
    "\n",
    "    # train with cv\n",
    "    cross_val_data = cross_val_score(\n",
    "        model, \n",
    "        X_train_pars, \n",
    "        y_train,\n",
    "        scoring='roc_auc', \n",
    "        cv=StratifiedKFold(n_splits=10, random_state=seed, shuffle=True), \n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # === IMPORTANT ===\n",
    "    # data to be returned by the search, we can add as much as we want\n",
    "    \n",
    "    loss = -cross_val_data.mean()\n",
    "    loss_variance = cross_val_data.std()\n",
    "    \n",
    "    try:\n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'loss_variance':loss_variance,\n",
    "            'status': STATUS_OK,\n",
    "            }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'exception': str(e),\n",
    "            'status': STATUS_FAIL,\n",
    "            }\n",
    "\n",
    "\n",
    "trials_tpe = Trials()\n",
    "\n",
    "tpe_search = fmin(\n",
    "    fn=objective,\n",
    "    space=param_grid,\n",
    "    max_evals=120,\n",
    "    rstate=np.random.RandomState(seed),\n",
    "    algo=tpe.suggest,  # tpe\n",
    "    trials=trials_tpe\n",
    ")\n",
    "\n",
    "print()\n",
    "print(f'Best Params: {tpe_search}')\n",
    "print()\n",
    "\n",
    "#Plots\n",
    "results = pd.concat([\n",
    "    pd.DataFrame(trials_tpe.vals),\n",
    "    pd.DataFrame(trials_tpe.results)],\n",
    "    axis=1,\n",
    ").sort_values(by='loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "results['index'] = results.index\n",
    "\n",
    "ax = sns.lineplot(x='index', y='loss', data=results)\n",
    "ax.fill_between(\n",
    "    results[\"index\"],\n",
    "    y1=results[\"loss\"] - results[\"loss_variance\"],\n",
    "    y2=results[\"loss\"] + results[\"loss_variance\"],\n",
    "    alpha=.5,\n",
    ")\n",
    "plt.xlabel('interation')\n",
    "plt.title('TPE Search')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To pass as dictionary\n",
    "\n",
    "def create_param_grid(search, booster): #Here, booster represents the cateogircal parameter.\n",
    "    best_hp_dict = {\n",
    "        'n_estimators': int(search['n_estimators']),\n",
    "        # important int, as it takes integers only\n",
    "        'max_depth': int(search['max_depth']),\n",
    "        'learning_rate': search['learning_rate'],\n",
    "        'booster': booster,\n",
    "        'gamma': search['gamma'],\n",
    "        'subsample': search['subsample'],\n",
    "        'colsample_bytree': search['colsample_bytree'],\n",
    "        'colsample_bylevel': search['colsample_bylevel'],\n",
    "        'colsample_bynode': search['colsample_bynode'],\n",
    "        'random_state': seed,\n",
    "        'reg_lambda': int(search['reg_lambda']), \n",
    "        'min_child_weight': int(search['min_child_weight']),\n",
    "        'base_score': search['base_score'], \n",
    "        'max_delta_step': int(search['max_delta_step']), \n",
    "        'reg_lambda': int(search['reg_lambda'])\n",
    "    }\n",
    "    return best_hp_dict\n",
    "\n",
    "xgb_param = create_param_grid(tpe_search, #Categorical columns 'gbtree')\n",
    "print(xgb_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# hp: define the hyperparameter space\n",
    "# fmin: optimization function\n",
    "# Trials: to evaluate the different searched hyperparameters\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from hyperopt import hp, fmin, Trials\n",
    "\n",
    "# the search algorithms\n",
    "from hyperopt import rand, anneal, tpe\n",
    "\n",
    "# for the search\n",
    "from hyperopt import STATUS_OK, STATUS_FAIL\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "param_grid = {\n",
    "    'iterations': hp.quniform('iterations', 200, 1000, 25),\n",
    "    'depth': hp.quniform('depth', 1, 7, 1),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.001), np.log(1)),\n",
    "    'boosting_type': hp.choice('boosting_type', ['Ordered', 'Plain']),\n",
    "    'l2_leaf_reg': hp.uniform('l2_leaf_reg', 1.0, 100.0),\n",
    "    'border_count': hp.loguniform('border_count', np.log(5), np.log(200))\n",
    "}\n",
    "\n",
    "#Defining Objective Function\n",
    "\n",
    "def objective(params):\n",
    "\n",
    "    # we need a dictionary to indicate which value from the space\n",
    "    # to attribute to each value of the hyperparameter in the xgb\n",
    "    params_dict = {\n",
    "        'iterations': int(params['iterations']),\n",
    "        'depth': int(params['depth']),\n",
    "        'learning_rate': params['learning_rate'],\n",
    "        'boosting_type': params['boosting_type'],\n",
    "        'l2_leaf_reg': params['l2_leaf_reg'],\n",
    "        'border_count': int(params['border_count']) \n",
    "    }\n",
    "\n",
    "    # with ** we pass the items in the dictionary as parameters\n",
    "    # to the xgb\n",
    "    \n",
    "    cat = CatBoostClassifier(verbose = False, loss_function='CrossEntropy', eval_metric='TotalF1', \n",
    "                             random_seed = seed)\n",
    "    \n",
    "    model = cat.set_params(**params_dict)\n",
    "\n",
    "    # train with cv\n",
    "    cross_val_data = cross_val_score(\n",
    "        model, \n",
    "        X_train_pars, \n",
    "        y_train,\n",
    "        scoring='roc_auc', \n",
    "        cv=StratifiedKFold(n_splits=10, random_state=seed, shuffle=True), \n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    \n",
    "    # === IMPORTANT ===\n",
    "    # data to be returned by the search, we can add as much as we want\n",
    "    \n",
    "    loss = -cross_val_data.mean()\n",
    "    loss_variance = cross_val_data.std()\n",
    "    \n",
    "    try:\n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'loss_variance':loss_variance,\n",
    "            'status': STATUS_OK,\n",
    "            }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'exception': str(e),\n",
    "            'status': STATUS_FAIL,\n",
    "            }\n",
    "\n",
    "\n",
    "trials_tpe = Trials()\n",
    "\n",
    "tpe_search = fmin(\n",
    "    fn=objective,\n",
    "    space=param_grid,\n",
    "    max_evals=120,\n",
    "    rstate=np.random.RandomState(seed),\n",
    "    algo=tpe.suggest,  # tpe\n",
    "    trials=trials_tpe\n",
    ")\n",
    "\n",
    "print()\n",
    "print(f'Best Params: {tpe_search}')\n",
    "print()\n",
    "\n",
    "#Plots\n",
    "results = pd.concat([\n",
    "    pd.DataFrame(trials_tpe.vals),\n",
    "    pd.DataFrame(trials_tpe.results)],\n",
    "    axis=1,\n",
    ").sort_values(by='loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "results['index'] = results.index\n",
    "\n",
    "ax = sns.lineplot(x='index', y='loss', data=results)\n",
    "ax.fill_between(\n",
    "    results[\"index\"],\n",
    "    y1=results[\"loss\"] - results[\"loss_variance\"],\n",
    "    y2=results[\"loss\"] + results[\"loss_variance\"],\n",
    "    alpha=.5,\n",
    ")\n",
    "plt.xlabel('interation')\n",
    "plt.title('TPE Search')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_param_grid(search, boosting_type):\n",
    "    best_hp_dict = {\n",
    "        'iterations': int(search['iterations']),\n",
    "        'depth': int(search['depth']),\n",
    "        'learning_rate': search['learning_rate'],\n",
    "        'boosting_type': boosting_type,\n",
    "        'l2_leaf_reg': search['l2_leaf_reg'],\n",
    "        'border_count': int(search['border_count'])\n",
    "    }\n",
    "    return best_hp_dict\n",
    "\n",
    "cat_param = create_param_grid(tpe_search, #boosting_type in string format)\n",
    "print(cat_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# hp: define the hyperparameter space\n",
    "# fmin: optimization function\n",
    "# Trials: to evaluate the different searched hyperparameters\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from hyperopt import hp, fmin, Trials\n",
    "\n",
    "# the search algorithms\n",
    "from hyperopt import rand, anneal, tpe\n",
    "\n",
    "# for the search\n",
    "from hyperopt import STATUS_OK, STATUS_FAIL\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': hp.quniform('n_estimators', 10, 2500, 25),\n",
    "    'max_samples': hp.uniform('max_samples', 0.50, 0.999),\n",
    "    'max_features': hp.uniform('max_features', 0.30, 0.999),\n",
    "    'bootstrap': hp.choice('bootstrap', [True, False]),\n",
    "    'bootstrap_features': hp.choice('bootstrap_features', [True, False]),\n",
    "    'warm_start': hp.choice('warm_start', [True, False])\n",
    "}\n",
    "\n",
    "#Defining Objective Function\n",
    "\n",
    "def objective(params):\n",
    "\n",
    "    # we need a dictionary to indicate which value from the space\n",
    "    # to attribute to each value of the hyperparameter in the xgb\n",
    "    params_dict = {\n",
    "        # important int, as it takes integers only\n",
    "        'n_estimators': int(params['n_estimators']),\n",
    "        'max_samples': params['max_samples'],\n",
    "        'max_features': params['max_features'],\n",
    "        'bootstrap': params['bootstrap'],\n",
    "        'bootstrap_features': params['bootstrap_features'],\n",
    "        'warm_start': params['warm_start']\n",
    "    }\n",
    "\n",
    "    # with ** we pass the items in the dictionary as parameters\n",
    "    # to the xgb\n",
    "    model = BaggingClassifier(n_jobs = -1, **params_dict)\n",
    "\n",
    "    # train with cv\n",
    "    cross_val_data = cross_val_score(\n",
    "        model, \n",
    "        X_train_pars, \n",
    "        y_train,\n",
    "        scoring='roc_auc', \n",
    "        cv=StratifiedKFold(n_splits=10, random_state=seed, shuffle=True), \n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    \n",
    "    # === IMPORTANT ===\n",
    "    # data to be returned by the search, we can add as much as we want\n",
    "    \n",
    "    score = -cross_val_data.mean()\n",
    "    score_variance = cross_val_data.std()\n",
    "    \n",
    "    try:\n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'loss_variance':loss_variance,\n",
    "            'status': STATUS_OK,\n",
    "            }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'exception': str(e),\n",
    "            'status': STATUS_FAIL,\n",
    "            }\n",
    "\n",
    "\n",
    "trials_tpe = Trials()\n",
    "\n",
    "tpe_search = fmin(\n",
    "    fn=objective,\n",
    "    space=param_grid,\n",
    "    max_evals=120,\n",
    "    rstate=np.random.RandomState(seed),\n",
    "    algo=tpe.suggest,  # tpe\n",
    "    trials=trials_tpe\n",
    ")\n",
    "\n",
    "print()\n",
    "print(f'Best Params: {tpe_search}')\n",
    "print()\n",
    "\n",
    "#Plots\n",
    "results = pd.concat([\n",
    "    pd.DataFrame(trials_tpe.vals),\n",
    "    pd.DataFrame(trials_tpe.results)],\n",
    "    axis=1,\n",
    ").sort_values(by='loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "results['index'] = results.index\n",
    "\n",
    "ax = sns.lineplot(x='index', y='loss', data=results)\n",
    "ax.fill_between(\n",
    "    results[\"index\"],\n",
    "    y1=results[\"loss\"] - results[\"loss_variance\"],\n",
    "    y2=results[\"loss\"] + results[\"loss_variance\"],\n",
    "    alpha=.5,\n",
    ")\n",
    "plt.xlabel('interation')\n",
    "plt.title('TPE Search')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_param_grid(search, bootstrap, bootstrap_features, warm_start):\n",
    "    best_hp_dict = {\n",
    "        'n_estimators': int(search['n_estimators']),\n",
    "        'max_samples': search['max_samples'],\n",
    "        'max_features': search['max_features'],\n",
    "        'bootstrap': bootstrap,\n",
    "        'bootstrap_features': bootstrap_features,\n",
    "        'warm_start': warm_start\n",
    "    }\n",
    "    return best_hp_dict\n",
    "\n",
    "bagC_param = create_param_grid(tpe_search, #bootstrap, bootstrap_features, warm_start in string format)\n",
    "print(bagC_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized Greedy Forest (RGF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# hp: define the hyperparameter space\n",
    "# fmin: optimization function\n",
    "# Trials: to evaluate the different searched hyperparameters\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from hyperopt import hp, fmin, Trials\n",
    "\n",
    "# the search algorithms\n",
    "from hyperopt import rand, anneal, tpe\n",
    "\n",
    "# for the search\n",
    "from hyperopt import STATUS_OK, STATUS_FAIL\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'algorithm':  hp.choice('algorithm', ['RGF', 'RGF_Opt', 'RGF_Sib']),\n",
    "    'calc_prob': hp.choice('calc_prob', [\"softmax\", \"sigmoid\"]),\n",
    "    'l2': hp.uniform('l2', 0.0001, 1),\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.001, 0.5),\n",
    "    'loss': hp.choice('loss', ['LS', 'Log', 'Expo']),\n",
    "    'max_leaf': hp.uniform('max_leaf', 1000, 10000),\n",
    "    'min_samples_leaf': hp.uniform('min_samples_leaf', 1, 20),\n",
    "    'reg_depth': hp.uniform('reg_depth', 1.0, 5.0),\n",
    "    'test_interval': hp.uniform('test_interval', 100, 600)\n",
    "}\n",
    "\n",
    "#Defining Objective Function\n",
    "\n",
    "def objective(params):\n",
    "\n",
    "    # we need a dictionary to indicate which value from the space\n",
    "    # to attribute to each value of the hyperparameter in the xgb\n",
    "    params_dict = {\n",
    "        'algorithm': params['algorithm'], \n",
    "        'calc_prob': params['calc_prob'], \n",
    "        'l2': params['l2'], \n",
    "        'learning_rate': params['learning_rate'], \n",
    "        'loss': params['loss'], \n",
    "        'max_leaf': int(params['max_leaf']),\n",
    "        'min_samples_leaf': int(params['min_samples_leaf']),\n",
    "        'reg_depth': params['reg_depth'], \n",
    "        'test_interval': int(params['test_interval'])\n",
    "    }\n",
    "\n",
    "    # with ** we pass the items in the dictionary as parameters\n",
    "    # to the xgb\n",
    "    \n",
    "    model = RGFClassifier().set_params(**params_dict)\n",
    "\n",
    "    # train with cv\n",
    "    cross_val_data = cross_val_score(\n",
    "        model, \n",
    "        X_train_pars, \n",
    "        y_train,\n",
    "        scoring='f1', \n",
    "        cv=StratifiedKFold(n_splits=10, random_state=seed, shuffle=True), \n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    \n",
    "    # === IMPORTANT ===\n",
    "    # data to be returned by the search, we can add as much as we want\n",
    "    \n",
    "    loss = -cross_val_data.mean()\n",
    "    loss_variance = cross_val_data.std()\n",
    "    \n",
    "    try:\n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'loss_variance':loss_variance,\n",
    "            'status': STATUS_OK,\n",
    "            }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'exception': str(e),\n",
    "            'status': STATUS_FAIL,\n",
    "            }\n",
    "\n",
    "\n",
    "trials_tpe = Trials()\n",
    "\n",
    "tpe_search = fmin(\n",
    "    fn=objective,\n",
    "    space=param_grid,\n",
    "    max_evals=120,\n",
    "    rstate=np.random.RandomState(seed),\n",
    "    algo=tpe.suggest,  # tpe\n",
    "    trials=trials_tpe\n",
    ")\n",
    "\n",
    "print()\n",
    "print(f'Best Params: {tpe_search}')\n",
    "print()\n",
    "\n",
    "#Plots\n",
    "results = pd.concat([\n",
    "    pd.DataFrame(trials_tpe.vals),\n",
    "    pd.DataFrame(trials_tpe.results)],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "results.columns = ['algorithm', 'calc_prob', 'l2', 'learning_rate', 'loss', 'max_leaf',\n",
    "                   'min_samples_leaf', 'reg_depth', 'test_interval', 'score',\n",
    "                   'score_variance', 'status']\n",
    "\n",
    "results = results.sort_values(by='score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "results['index'] = results.index\n",
    "\n",
    "ax = sns.lineplot(x='index', y='score', data=results)\n",
    "ax.fill_between(\n",
    "    results[\"index\"],\n",
    "    y1=results[\"score\"] - results[\"score_variance\"],\n",
    "    y2=results[\"score\"] + results[\"score_variance\"],\n",
    "    alpha=.5,\n",
    ")\n",
    "plt.xlabel('interation')\n",
    "plt.title('TPE Search')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_param_grid(search, algorithm, calc_prob, loss):\n",
    "    best_hp_dict = {\n",
    "        'algorithm': algorithm, \n",
    "        'calc_prob': calc_prob, \n",
    "        'l2': search['l2'], \n",
    "        'learning_rate': search['learning_rate'], \n",
    "        'loss': loss, \n",
    "        'max_leaf': int(search['max_leaf']),\n",
    "        'min_samples_leaf': int(search['min_samples_leaf']),\n",
    "        'reg_depth': search['reg_depth'], \n",
    "        'test_interval': int(search['test_interval'])\n",
    "    }\n",
    "    return best_hp_dict\n",
    "\n",
    "rgf_param = create_param_grid(tpe_search, #algorithm, calc_prob, loss in string format)\n",
    "print(rgf_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# hp: define the hyperparameter space\n",
    "# fmin: optimization function\n",
    "# Trials: to evaluate the different searched hyperparameters\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from hyperopt import hp, fmin, Trials\n",
    "\n",
    "# the search algorithms\n",
    "from hyperopt import rand, anneal, tpe\n",
    "\n",
    "# for the search\n",
    "from hyperopt import STATUS_OK, STATUS_FAIL\n",
    "\n",
    "param_grid = {\n",
    "    'boosting_type' : hp.choice('boosting_type', ['gbdt', 'dart', 'goss', 'rf']), \n",
    "#     'class_weight' : hp.choice(['balanced']), \n",
    "    'colsample_bytree' : hp.uniform('colsample_bytree', 0.50, 1.0),\n",
    "    'importance_type' : hp.choice('importance_type', ['split', 'gain']), \n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.001), np.log(1)),\n",
    "    'max_depth': hp.quniform('max_depth', 1, 6, 1),\n",
    "    'min_child_weight' : hp.loguniform('min_child_weight', np.log(0.001), np.log(1)),\n",
    "    'n_estimators': hp.quniform('n_estimators', 100, 2500, 25),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0.0001, 2.0),\n",
    "    'reg_lambda': hp.loguniform('reg_lambda', np.log(1), np.log(20)),\n",
    "    'subsample': hp.uniform('subsample', 0.50, 0.90)\n",
    "}\n",
    "\n",
    "#Defining Objective Function\n",
    "\n",
    "def objective(params):\n",
    "\n",
    "    # we need a dictionary to indicate which value from the space\n",
    "    # to attribute to each value of the hyperparameter in the xgb\n",
    "    params_dict = {\n",
    "        'boosting_type': params['boosting_type'],\n",
    "        'class_weight': 'balanced',\n",
    "        'colsample_bytree': params['colsample_bytree'],\n",
    "        'importance_type': params['importance_type'],\n",
    "        'learning_rate': params['learning_rate'],\n",
    "        'max_depth': int(params['max_depth']),\n",
    "        'min_child_weight': params['min_child_weight'],\n",
    "        'n_estimators': int(params['n_estimators']),\n",
    "        'reg_alpha': params['reg_alpha'],\n",
    "        'reg_lambda': params['reg_lambda'],\n",
    "        'subsample': params['subsample'], \n",
    "        'random_state': seed\n",
    "    }\n",
    "\n",
    "    # with ** we pass the items in the dictionary as parameters\n",
    "    # to the xgb\n",
    "    model = GPBoostClassifier(**params_dict)\n",
    "\n",
    "    # train with cv\n",
    "    cross_val_data = cross_val_score(\n",
    "        model, \n",
    "        X_train_pars, \n",
    "        y_train,\n",
    "        scoring='f1', \n",
    "        cv=StratifiedKFold(n_splits=10, random_state=seed, shuffle=True), \n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # === IMPORTANT ===\n",
    "    # data to be returned by the search, we can add as much as we want\n",
    "    \n",
    "    loss = -cross_val_data.mean()\n",
    "    loss_variance = cross_val_data.std()\n",
    "    \n",
    "    try:\n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'loss_variance':loss_variance,\n",
    "            'status': STATUS_OK,\n",
    "            }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'exception': str(e),\n",
    "            'status': STATUS_FAIL,\n",
    "            }\n",
    "\n",
    "\n",
    "trials_tpe = Trials()\n",
    "\n",
    "tpe_search = fmin(\n",
    "    fn=objective,\n",
    "    space=param_grid,\n",
    "    max_evals=120,\n",
    "    rstate=np.random.RandomState(seed),\n",
    "    algo=tpe.suggest,  # tpe\n",
    "    trials=trials_tpe\n",
    ")\n",
    "\n",
    "print()\n",
    "print(f'Best Params: {tpe_search}')\n",
    "print()\n",
    "\n",
    "#Plots\n",
    "results = pd.concat([\n",
    "    pd.DataFrame(trials_tpe.vals),\n",
    "    pd.DataFrame(trials_tpe.results)],\n",
    "    axis=1,\n",
    ").sort_values(by='loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "results['index'] = results.index\n",
    "\n",
    "ax = sns.lineplot(x='index', y='loss', data=results)\n",
    "ax.fill_between(\n",
    "    results[\"index\"],\n",
    "    y1=results[\"loss\"] - results[\"loss_variance\"],\n",
    "    y2=results[\"loss\"] + results[\"loss_variance\"],\n",
    "    alpha=.5,\n",
    ")\n",
    "plt.xlabel('interation')\n",
    "plt.title('TPE Search')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To pass as dictionary\n",
    "\n",
    "def create_param_grid(search, boosting_type, importance_type): \n",
    "    best_hp_dict = {\n",
    "        'boosting_type': boosting_type,\n",
    "        'class_weight': 'balanced',\n",
    "        'colsample_bytree': search['colsample_bytree'],\n",
    "        'importance_type': importance_type,\n",
    "        'learning_rate': search['learning_rate'],\n",
    "        'max_depth': int(search['max_depth']),\n",
    "        'min_child_weight': search['min_child_weight'],\n",
    "        'n_estimators': int(search['n_estimators']),\n",
    "        'reg_alpha': search['reg_alpha'],\n",
    "        'reg_lambda': search['reg_lambda'],\n",
    "        'subsample': search['subsample'], \n",
    "        'random_state': seed\n",
    "    }\n",
    "    return best_hp_dict\n",
    "\n",
    "gpb_param = create_param_grid(tpe_search, 'gbdt', 'gain')\n",
    "print(gpb_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# hp: define the hyperparameter space\n",
    "# fmin: optimization function\n",
    "# Trials: to evaluate the different searched hyperparameters\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from hyperopt import hp, fmin, Trials\n",
    "\n",
    "# the search algorithms\n",
    "from hyperopt import rand, anneal, tpe\n",
    "\n",
    "# for the search\n",
    "from hyperopt import STATUS_OK, STATUS_FAIL\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': hp.quniform('n_estimators', 100, 500, 25),\n",
    "    'criterion' : hp.choice('criterion', ['gini', 'entropy']),\n",
    "    'max_depth': hp.quniform('max_depth', 1, 6, 1),\n",
    "    'min_samples_split': hp.uniform('min_samples_split', 0.0, 1.0),\n",
    "    'min_samples_leaf': hp.uniform('min_samples_leaf', 1, 30),\n",
    "    'max_features' : hp.choice('max_features', ['sqrt', 'log2']), \n",
    "    'ccp_alpha': hp.uniform('ccp_alpha', 0.0, 0.035),\n",
    "    'bootstrap' : hp.choice('bootstrap', [True]),\n",
    "    'class_weight' : hp.choice('class_weight', ['balanced', 'balanced_subsample']) \n",
    "}\n",
    "\n",
    "#Defining Objective Function\n",
    "\n",
    "def objective(params):\n",
    "\n",
    "    # we need a dictionary to indicate which value from the space\n",
    "    # to attribute to each value of the hyperparameter in the xgb\n",
    "    params_dict = {\n",
    "        'n_estimators': int(params['n_estimators']),\n",
    "        'criterion': params['criterion'],\n",
    "        'max_depth': int(params['max_depth']),\n",
    "        'min_samples_split': params['min_samples_split'],\n",
    "        'min_samples_leaf': int(params['min_samples_leaf']),\n",
    "        'max_features': params['max_features'], \n",
    "        'ccp_alpha': params['ccp_alpha'], \n",
    "        'bootstrap': params['bootstrap'], \n",
    "        'class_weight': params['class_weight'],\n",
    "        'random_state': seed\n",
    "    }\n",
    "\n",
    "    # with ** we pass the items in the dictionary as parameters\n",
    "    # to the xgb\n",
    "    model = RandomForestClassifier(**params_dict)\n",
    "\n",
    "    # train with cv\n",
    "    cross_val_data = cross_val_score(\n",
    "        model, \n",
    "        X_train_pars, \n",
    "        y_train,\n",
    "        scoring='f1', \n",
    "        cv=StratifiedKFold(n_splits=10, random_state=seed, shuffle=True), \n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # === IMPORTANT ===\n",
    "    # data to be returned by the search, we can add as much as we want\n",
    "    \n",
    "    loss = -cross_val_data.mean()\n",
    "    loss_variance = cross_val_data.std()\n",
    "    \n",
    "    try:\n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'loss_variance':loss_variance,\n",
    "            'status': STATUS_OK,\n",
    "            }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'exception': str(e),\n",
    "            'status': STATUS_FAIL,\n",
    "            }\n",
    "\n",
    "\n",
    "trials_tpe = Trials()\n",
    "\n",
    "tpe_search = fmin(\n",
    "    fn=objective,\n",
    "    space=param_grid,\n",
    "    max_evals=150,\n",
    "    rstate=np.random.RandomState(seed),\n",
    "    algo=tpe.suggest,  # tpe\n",
    "    trials=trials_tpe\n",
    ")\n",
    "\n",
    "print()\n",
    "print(f'Best Params: {tpe_search}')\n",
    "print()\n",
    "\n",
    "#Plots\n",
    "results = pd.concat([\n",
    "    pd.DataFrame(trials_tpe.vals),\n",
    "    pd.DataFrame(trials_tpe.results)],\n",
    "    axis=1,\n",
    ").sort_values(by='loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "results['index'] = results.index\n",
    "\n",
    "ax = sns.lineplot(x='index', y='loss', data=results)\n",
    "ax.fill_between(\n",
    "    results[\"index\"],\n",
    "    y1=results[\"loss\"] - results[\"loss_variance\"],\n",
    "    y2=results[\"loss\"] + results[\"loss_variance\"],\n",
    "    alpha=.5,\n",
    ")\n",
    "plt.xlabel('interation')\n",
    "plt.title('TPE Search')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To pass as dictionary\n",
    "\n",
    "def create_param_grid(search, criterion, max_features, bootstrap, class_weight): \n",
    "    best_hp_dict = {\n",
    "        'n_estimators': int(search['n_estimators']),\n",
    "        'criterion': criterion,\n",
    "        'max_depth': int(search['max_depth']),\n",
    "        'min_samples_split': search['min_samples_split'],\n",
    "        'min_samples_leaf': int(search['min_samples_leaf']),\n",
    "        'max_features': max_features, \n",
    "        'ccp_alpha': search['ccp_alpha'], \n",
    "        'bootstrap': bootstrap, \n",
    "        'class_weight': class_weight,\n",
    "        'random_state': seed\n",
    "    }\n",
    "    return best_hp_dict\n",
    "\n",
    "rf_param = create_param_grid(tpe_search, #criterion, max_features, bootstrap, class_weight)\n",
    "print(rf_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ExtraTrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# hp: define the hyperparameter space\n",
    "# fmin: optimization function\n",
    "# Trials: to evaluate the different searched hyperparameters\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from hyperopt import hp, fmin, Trials\n",
    "\n",
    "# the search algorithms\n",
    "from hyperopt import rand, anneal, tpe\n",
    "\n",
    "# for the search\n",
    "from hyperopt import STATUS_OK, STATUS_FAIL\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': hp.quniform('n_estimators', 100, 500, 25),\n",
    "    'criterion' : hp.choice('criterion', ['gini', 'entropy']),\n",
    "    'max_depth': hp.quniform('max_depth', 1, 6, 1),\n",
    "    'min_samples_split': hp.uniform('min_samples_split', 0.0, 1.0),\n",
    "    'min_samples_leaf': hp.uniform('min_samples_leaf', 1, 30),\n",
    "    'max_features' : hp.choice('max_features', ['sqrt', 'log2']), \n",
    "    'ccp_alpha': hp.uniform('ccp_alpha', 0.0, 0.035),\n",
    "    'bootstrap' : hp.choice('bootstrap', [True]),\n",
    "    'class_weight' : hp.choice('class_weight', ['balanced', 'balanced_subsample']) \n",
    "}\n",
    "\n",
    "#Defining Objective Function\n",
    "\n",
    "def objective(params):\n",
    "\n",
    "    # we need a dictionary to indicate which value from the space\n",
    "    # to attribute to each value of the hyperparameter in the xgb\n",
    "    params_dict = {\n",
    "        'n_estimators': int(params['n_estimators']),\n",
    "        'criterion': params['criterion'],\n",
    "        'max_depth': int(params['max_depth']),\n",
    "        'min_samples_split': params['min_samples_split'],\n",
    "        'min_samples_leaf': int(params['min_samples_leaf']),\n",
    "        'max_features': params['max_features'], \n",
    "        'ccp_alpha': params['ccp_alpha'], \n",
    "        'bootstrap': params['bootstrap'], \n",
    "        'class_weight': params['class_weight'],\n",
    "        'random_state': seed\n",
    "    }\n",
    "\n",
    "    # with ** we pass the items in the dictionary as parameters\n",
    "    # to the xgb\n",
    "    model = ExtraTreesClassifier(**params_dict)\n",
    "\n",
    "    # train with cv\n",
    "    cross_val_data = cross_val_score(\n",
    "        model, \n",
    "        X_train_pars, \n",
    "        y_train,\n",
    "        scoring='f1', \n",
    "        cv=StratifiedKFold(n_splits=10, random_state=seed, shuffle=True), \n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # === IMPORTANT ===\n",
    "    # data to be returned by the search, we can add as much as we want\n",
    "    \n",
    "    loss = -cross_val_data.mean()\n",
    "    loss_variance = cross_val_data.std()\n",
    "    \n",
    "    try:\n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'loss_variance':loss_variance,\n",
    "            'status': STATUS_OK,\n",
    "            }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'exception': str(e),\n",
    "            'status': STATUS_FAIL,\n",
    "            }\n",
    "\n",
    "\n",
    "trials_tpe = Trials()\n",
    "\n",
    "tpe_search = fmin(\n",
    "    fn=objective,\n",
    "    space=param_grid,\n",
    "    max_evals=150,\n",
    "    rstate=np.random.RandomState(seed),\n",
    "    algo=tpe.suggest,  # tpe\n",
    "    trials=trials_tpe\n",
    ")\n",
    "\n",
    "print()\n",
    "print(f'Best Params: {tpe_search}')\n",
    "print()\n",
    "\n",
    "#Plots\n",
    "results = pd.concat([\n",
    "    pd.DataFrame(trials_tpe.vals),\n",
    "    pd.DataFrame(trials_tpe.results)],\n",
    "    axis=1,\n",
    ").sort_values(by='loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "results['index'] = results.index\n",
    "\n",
    "ax = sns.lineplot(x='index', y='loss', data=results)\n",
    "ax.fill_between(\n",
    "    results[\"index\"],\n",
    "    y1=results[\"loss\"] - results[\"loss_variance\"],\n",
    "    y2=results[\"loss\"] + results[\"loss_variance\"],\n",
    "    alpha=.5,\n",
    ")\n",
    "plt.xlabel('interation')\n",
    "plt.title('TPE Search')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To pass as dictionary\n",
    "\n",
    "def create_param_grid(search, criterion, max_features, bootstrap, class_weight): \n",
    "    best_hp_dict = {\n",
    "        'n_estimators': int(search['n_estimators']),\n",
    "        'criterion': criterion,\n",
    "        'max_depth': int(search['max_depth']),\n",
    "        'min_samples_split': search['min_samples_split'],\n",
    "        'min_samples_leaf': int(search['min_samples_leaf']),\n",
    "        'max_features': max_features, \n",
    "        'ccp_alpha': search['ccp_alpha'], \n",
    "        'bootstrap': bootstrap, \n",
    "        'class_weight': class_weight,\n",
    "        'random_state': seed\n",
    "    }\n",
    "    return best_hp_dict\n",
    "\n",
    "et_param = create_param_grid(tpe_search, #criterion, max_features, bootstrap, class_weight)\n",
    "print(et_param)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
