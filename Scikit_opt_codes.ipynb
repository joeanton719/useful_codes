{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HashableDict(dict):\n",
    "    def __hash__(self):\n",
    "        return hash(tuple(sorted(self.items())))\n",
    "\n",
    "## search space\n",
    " sgd_search_params = {\n",
    "   'clf__loss': Categorical(['log', 'modified_huber']),\n",
    "   'clf__class_weight': Categorical([HashableDict({0: 0.3, 1: 0}), HashableDict({0: 0.8, 1: 0}))\n",
    "}\n",
    "##https://github.com/scikit-optimize/scikit-optimize/issues/681#issuecomment-781879964"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balanced RandomForest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Balanced Random Forest Classifier\n",
    "%%time\n",
    "from skopt import BayesSearchCV\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from skopt.space import Real, Integer, Categorical\n",
    " \n",
    "from collections import OrderedDict\n",
    "\n",
    "params = [{\n",
    "    'n_estimators': Integer(10,1500),\n",
    "    'criterion': Categorical(['gini', 'entropy']),\n",
    "    'max_depth': Integer(6,7),\n",
    "    'min_samples_split': Real(0.001, 0.99, 'log-uniform'), \n",
    "    'min_samples_leaf': Integer(1,10), \n",
    "    'max_features': Categorical(['sqrt', 'log2']),\n",
    "    'ccp_alpha': Real(0.0, 0.035, 'log-uniform'), \n",
    "    'bootstrap': Categorical([True]), \n",
    "    'replacement': Categorical([True]),\n",
    "    'class_weight': Categorical(['balanced', 'balanced_subsample']), \n",
    "    'max_samples': Real(0.1, 0.999, 'log-uniform')\n",
    "}]\n",
    "\n",
    "bayes_search = BayesSearchCV(estimator = BalancedRandomForestClassifier(), \n",
    "                             search_spaces = params, \n",
    "                             cv = StratifiedKFold(n_splits=5, random_state=seed, shuffle=True), \n",
    "                             scoring = 'f1', \n",
    "                             n_jobs = -1) #To use all processors\n",
    "\n",
    "tuning = 1\n",
    "\n",
    "while tuning <=5:\n",
    "     \n",
    "    bayes_search.fit(X_train_pars_df, y_train_df)\n",
    "    best_accuracy = bayes_search.best_score_\n",
    "    best_param = bayes_search.best_params_\n",
    "    print(f'tuning: {tuning},  \n",
    "    print(f'Best Accuracy: {best_accuracy*100} %')\n",
    "    print(f'Best Parameters: {best_param}')\n",
    "    print()\n",
    "    tuning += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "from skopt import gp_minimize\n",
    "from skopt.plots import plot_convergence\n",
    "from skopt.space import Real, Integer, Categorical #Real = float\n",
    "from skopt.utils import use_named_args\n",
    "\n",
    " \n",
    "\n",
    "param_grid = [\n",
    "    Integer(10, 1500, name=\"n_estimators\"),\n",
    "    Categorical(['gini', 'entropy'], name = \"criterion\"),\n",
    "    Integer(1,7, name = \"max_depth\"),\n",
    "    Real(0.0001, 0.999, name = \"min_samples_split\"),\n",
    "    Integer(1,30, name = \"min_samples_leaf\"),\n",
    "    Categorical(['sqrt', 'log2'], name = \"max_features\"),\n",
    "    Real(0.0001, 0.035, name=\"ccp_alpha\"),\n",
    "    Categorical([True, False], name = \"bootstrap\"), \n",
    "    Categorical([True, False], name = \"replacement\"), \n",
    "    Categorical(['balanced', 'balanced_subsample'], name = \"class_weight\"),\n",
    "    Real(0.1, 0.999, name=\"max_samples\")\n",
    "]\n",
    "\n",
    "model = BalancedRandomForestClassifier()\n",
    "\n",
    "@use_named_args(param_grid)\n",
    "def objective(**params):\n",
    "    \n",
    "    # model with new parameters\n",
    "    model.set_params(**params)\n",
    "\n",
    "    # optimization function (hyperparam response function)\n",
    "    value = np.mean(cross_val_score(model, X_train_pars_df, y_train_df,\n",
    "                                    cv=StratifiedKFold(n_splits=5, random_state=seed, shuffle=True),\n",
    "                                    n_jobs=-1, scoring='f1')\n",
    "                   )\n",
    "\n",
    "    # negate because we need to minimize\n",
    "    return -value\n",
    "\n",
    " \n",
    "\n",
    "gp_ = gp_minimize(\n",
    "    objective, # the objective function to minimize\n",
    "    param_grid, # the hyperparameter space\n",
    "    n_initial_points=20, # the number of points to evaluate f(x) to start of\n",
    "    acq_func='EI', # the acquisition function\n",
    "    n_calls=100, # the number of subsequent evaluations of f(x)\n",
    "    random_state=seed, \n",
    "    n_jobs = -1\n",
    ")\n",
    "\n",
    "print(f'Best Accuracy: {gp_.fun*100} %')  \n",
    "\n",
    "print(f\"\"\"Best parameters:\n",
    "=========================\n",
    "n_estimators={gp_.x[0]}, \n",
    "criterion='{gp_.x[1]}',\n",
    "max_depth={gp_.x[2]},\n",
    "min_samples_split={gp_.x[3]:.3f}, \n",
    "min_samples_leaf={gp_.x[4]}, \n",
    "max_features='{gp_.x[5]}', \n",
    "ccp_alpha={gp_.x[6]:.4f}, \n",
    "bootstrap={gp_.x[7]}, \n",
    "replacement={gp_.x[8]}, \n",
    "class_weight='{gp_.x[9]}', \n",
    "max_samples={gp_.x[10]:.3f} \"\"\") \n",
    "print()\n",
    "\n",
    "plot_convergence(gp_);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "%%time\n",
    "from skopt import BayesSearchCV\n",
    " from skopt.space import Real, Integer, Categorical\n",
    "from collections import OrderedDict\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "params = [{\n",
    "    'n_estimators': Integer(10, 1500), \n",
    "    'criterion': Categorical(['gini', 'entropy']),\n",
    "    'max_depth': Integer(1,7),\n",
    "    'min_samples_split': Real(0.0001, 0.999),\n",
    "    'min_samples_leaf': Integer(1,30),\n",
    "    'max_features': Categorical(['auto', 'sqrt', 'log2']),\n",
    "    'ccp_alpha': Real(0.0001, 0.035),\n",
    "    'bootstrap': Categorical([True, False]),\n",
    "    'class_weight': Categorical(['balanced', 'balanced_subsample'])\n",
    "}]\n",
    "\n",
    "bayes_search = BayesSearchCV(estimator = RandomForestClassifier(), \n",
    "                             search_spaces = params, \n",
    "                             cv = StratifiedKFold(n_splits=10, random_state=seed, shuffle=True), \n",
    "                             scoring = 'f1', \n",
    "                             n_jobs = -1) #To use all processors\n",
    "\n",
    "tuning = 1\n",
    "\n",
    "while tuning <=5:\n",
    "     \n",
    "    bayes_search.fit(X_train_pars, y_train)\n",
    "    best_accuracy = bayes_search.best_score_\n",
    "    best_param = bayes_search.best_params_\n",
    "    print(f'tuning: {tuning},  \n",
    "    print(f'Best Accuracy: {best_accuracy*100} %')\n",
    "    print(f'Best Parameters: {best_param}')\n",
    "    print()\n",
    "    tuning += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from skopt import gp_minimize\n",
    "from skopt.plots import plot_convergence\n",
    "from skopt.space import Real, Integer, Categorical #Real = float\n",
    "from skopt.utils import use_named_args\n",
    "\n",
    "param_grid = [\n",
    "    Integer(10, 1500, name=\"n_estimators\"),\n",
    "    Categorical(['gini', 'entropy'], name = \"criterion\"),\n",
    "    Integer(1,7, name = \"max_depth\"),\n",
    "    Real(0.0001, 0.999, name = \"min_samples_split\"),\n",
    "    Integer(1,30, name = \"min_samples_leaf\"),\n",
    "    Categorical(['sqrt', 'log2'], name = \"max_features\"),\n",
    "    Real(0.0001, 0.035, name=\"ccp_alpha\"),\n",
    "    Categorical([True, False], name = \"bootstrap\"),\n",
    "    Categorical(['balanced', 'balanced_subsample'], name = \"class_weight\")\n",
    "]\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "@use_named_args(param_grid)\n",
    "def objective(**params):\n",
    "    \n",
    "    # model with new parameters\n",
    "    model.set_params(**params)\n",
    "\n",
    "    # optimization function (hyperparam response function)\n",
    "    value = np.mean(cross_val_score(model, #X_train_pars, y_train,\n",
    "                                    cv=StratifiedKFold(n_splits=10, random_state=seed, shuffle=True),\n",
    "                                    n_jobs=-1, scoring='f1')\n",
    "                   )\n",
    "\n",
    "    # negate because we need to minimize\n",
    "    return -value\n",
    "\n",
    " \n",
    "\n",
    "gp_ = gp_minimize(\n",
    "    objective, # the objective function to minimize\n",
    "    param_grid, # the hyperparameter space\n",
    "    n_initial_points=20, # the number of points to evaluate f(x) to start of\n",
    "    acq_func='EI', # the acquisition function\n",
    "    n_calls=120, # the number of subsequent evaluations of f(x)\n",
    "    random_state=seed, \n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "print(f'Best Accuracy: {gp_.fun*100} %')  \n",
    "\n",
    "print(f\"\"\"Best parameters:\n",
    "=========================\n",
    "n_estimators={gp_.x[0]}, \n",
    "criterion='{gp_.x[1]}',\n",
    "max_depth={gp_.x[2]},\n",
    "min_samples_split={gp_.x[3]:.5f}, \n",
    "min_samples_leaf={gp_.x[4]}, \n",
    "max_features='{gp_.x[5]}', \n",
    "ccp_alpha={gp_.x[6]:.5f}, \n",
    "bootstrap={gp_.x[7]}, \n",
    "class_weight='{gp_.x[8]}' \"\"\") \n",
    "print()\n",
    "\n",
    "plot_convergence(gp_);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ExtraTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from skopt import gp_minimize\n",
    "from skopt.plots import plot_convergence\n",
    "from skopt.space import Real, Integer, Categorical #Real = float\n",
    "from skopt.utils import use_named_args\n",
    "\n",
    "param_grid = [\n",
    "    Integer(10, 2000, name=\"n_estimators\"),\n",
    "    Categorical(['gini', 'entropy'], name = \"criterion\"),\n",
    "    Integer(1,7, name = \"max_depth\"),\n",
    "    Real(0.0001, 0.999, name = \"min_samples_split\"),\n",
    "    Integer(1,30, name = \"min_samples_leaf\"),\n",
    "    Categorical(['sqrt', 'log2'], name = \"max_features\"),\n",
    "    Real(0.0001, 0.035, name=\"ccp_alpha\"),\n",
    "    Categorical([True, False], name = \"bootstrap\"),\n",
    "    Categorical(['balanced', 'balanced_subsample'], name = \"class_weight\")\n",
    "]\n",
    "\n",
    "model = ExtraTreesClassifier(n_jobs=-1)\n",
    "\n",
    "@use_named_args(param_grid)\n",
    "def objective(**params):\n",
    "    \n",
    "    # model with new parameters\n",
    "    model.set_params(**params)\n",
    "\n",
    "    # optimization function (hyperparam response function)\n",
    "    value = np.mean(cross_val_score(model, X_train_pars, y_train,\n",
    "                                    cv=StratifiedKFold(n_splits=10, random_state=seed, shuffle=True),\n",
    "                                    n_jobs=-1, \n",
    "                                    scoring='roc_auc')\n",
    "                   )\n",
    "\n",
    "    # negate because we need to minimize\n",
    "    return -value\n",
    "\n",
    " \n",
    "\n",
    "gp_ = gp_minimize(\n",
    "    objective, # the objective function to minimize\n",
    "    param_grid, # the hyperparameter space\n",
    "    n_initial_points=20, # the number of points to evaluate f(x) to start of\n",
    "    acq_func='EI', # the acquisition function\n",
    "    n_calls=120, # the number of subsequent evaluations of f(x)\n",
    "    random_state=seed, \n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "print(f'Best Accuracy: {gp_.fun*100} %')  \n",
    "\n",
    "print(f\"\"\"Best parameters:\n",
    "=========================\n",
    "n_estimators={gp_.x[0]}, \n",
    "criterion='{gp_.x[1]}',\n",
    "max_depth={gp_.x[2]},\n",
    "min_samples_split={gp_.x[3]:.5f}, \n",
    "min_samples_leaf={gp_.x[4]}, \n",
    "max_features='{gp_.x[5]}', \n",
    "ccp_alpha={gp_.x[6]:.5f}, \n",
    "bootstrap={gp_.x[7]}, \n",
    "class_weight='{gp_.x[8]}' \"\"\")\n",
    "print()\n",
    "\n",
    "plot_convergence(gp_);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from skopt import gp_minimize\n",
    "from skopt.plots import plot_convergence\n",
    "from skopt.space import Real, Integer, Categorical #Real = float\n",
    "from skopt.utils import use_named_args\n",
    " \n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "param_grid = [\n",
    "    Integer(1, 7, name=\"depth\"),\n",
    "    Integer(200, 1000, name=\"iterations\"),\n",
    "    Real(0.001, 0.4, name = \"learning_rate\"),\n",
    "    Real(1.0, 100.0, name = \"l2_leaf_reg\"),\n",
    "    Integer(5, 200, name=\"border_count\"),\n",
    "    Categorical(['Ordered', 'Plain'], name = \"boosting_type\")\n",
    "]\n",
    "\n",
    "cat = CatBoostClassifier(verbose = False, loss_function='CrossEntropy', eval_metric='TotalF1')\n",
    "\n",
    "@use_named_args(param_grid)\n",
    "def objective(**params):\n",
    "    \n",
    "    # model with new parameters\n",
    "    cat.set_params(**params)\n",
    "\n",
    "    # optimization function (hyperparam response function)\n",
    "    value = np.mean(cross_val_score(cat, #X_train_pars_df, y_train_df,\n",
    "                                    cv=StratifiedKFold(n_splits=10, random_state=seed, shuffle=True),\n",
    "                                    n_jobs=-1, scoring='f1')\n",
    "                   )\n",
    "\n",
    "    # negate because we need to minimize\n",
    "    return -value\n",
    "\n",
    " \n",
    "\n",
    "gp_ = gp_minimize(\n",
    "    objective, # the objective function to minimize\n",
    "    param_grid, # the hyperparameter space\n",
    "    n_initial_points=20, # the number of points to evaluate f(x) to start of\n",
    "    acq_func='EI', # the acquisition function\n",
    "    n_calls=100, # the number of subsequent evaluations of f(x)\n",
    "    random_state=seed, \n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "print(f'Best Accuracy: {gp_.fun*100} %')  \n",
    "\n",
    "print(f\"\"\"Best parameters:\n",
    "=========================\n",
    "depth={gp_.x[0]}, \n",
    "iterations={gp_.x[1]},\n",
    "learning_rate={gp_.x[2]:.5f}, \n",
    "l2_leaf_reg={gp_.x[3]:.5f},\n",
    "border_count={gp_.x[4]}, \n",
    "boosting_type='{gp_.x[5]}' \"\"\")\n",
    "print()\n",
    "\n",
    "plot_convergence(gp_);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from xgboost import XGBClassifier\n",
    "from collections import OrderedDict\n",
    "\n",
    "params = [{\n",
    "    'booster': Categorical(['dart', 'gbtree', 'gblinear']), \n",
    "    'learning_rate': Real(0.001, 0.15, prior = 'log-uniform'),\n",
    "    'max_depth': Integer(3,7),\n",
    "    'min_child_weight' : Integer(1,20, prior = 'log-uniform'),\n",
    "    'gamma' : Real(0.001,5.0, prior = 'log-uniform'),\n",
    "    'subsample': Real(0.4,0.999),\n",
    "    'colsample_bytree': Real(0.3,0.999),\n",
    "    'n_estimators' : Integer(10,1500), \n",
    "    'base_score': Real(0.3,0.65),\n",
    "    'max_delta_step': Integer(0,10, prior = 'log-uniform'),\n",
    "    'reg_alpha': Real(0.0001,2),\n",
    "    'reg_lambda': Integer(1, 50)\n",
    "}]\n",
    "\n",
    "bayes_search = BayesSearchCV(estimator = XGBClassifier(use_label_encoder=False, \n",
    "                                                       eval_metric = 'error', \n",
    "                                                       objective = 'binary:logistic', \n",
    "                                                       n_jobs = -1),\n",
    "                             search_spaces = params, \n",
    "                             cv = StratifiedKFold(n_splits=10, random_state=seed, shuffle = True), \n",
    "                             scoring = 'roc_auc',\n",
    "                             optimizer_kwargs = {'base_estimator': 'GP', \n",
    "                                                 'n_initial_points' : 20, \n",
    "                                                 'acq_func': 'EI', \n",
    "                                                 'n_jobs': -1},\n",
    "                             n_jobs = -1) #To use all processors\n",
    "\n",
    "tuning = 1\n",
    "\n",
    "while tuning <2:\n",
    "    bayes_search.fit(X_train_pars, y_train)\n",
    "    best_accuracy = bayes_search.best_score_\n",
    "    best_param = bayes_search.best_params_\n",
    "    print(f'tuning: {tuning}')\n",
    "    print(f'Best Accuracy: {best_accuracy*100:.4f} %')\n",
    "    print(f'Best Parameters: {best_param}')\n",
    "    print()\n",
    "    tuning += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
